{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb177750-f613-45ce-bb43-e3d46d7d1bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489f009-c39a-4ae2-aa40-b238058e29a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c7f1eb-f2da-4e2e-ab88-ec7db6034a83",
   "metadata": {},
   "source": [
    "### Breast Cancer Dataset Explanation\n",
    "\n",
    "The **Breast Cancer dataset** is a widely-used binary classification dataset that comes from the **sklearn.datasets** module. It contains **569 samples** of data on breast cancer patients, with each sample corresponding to a patient's tumor. Each sample is described by **30 numeric features**, which represent various characteristics of the cell nuclei present in a biopsy. These features include attributes such as the **mean radius**, **texture**, **perimeter**, **area**, and **smoothness** of the cell nuclei.\n",
    "\n",
    "The target variable is binary, indicating whether the cancer is **benign (label 1)** or **malignant (label 0)**. The goal of the dataset is to predict whether a given tumor is benign or malignant based on the input features. It is a well-balanced dataset with **357 benign** and **212 malignant** instances.\n",
    "\n",
    "In addition to the feature data and target labels, the **Breast Cancer dataset** object from `sklearn` contains the following useful components:\n",
    "\n",
    "1. **`data`:** A 2D array of shape (569, 30) containing the feature values for each sample (30 features per sample).\n",
    "2. **`target`:** A 1D array of shape (569,) with the binary target values (0 for malignant, 1 for benign).\n",
    "3. **`target_names`:** The array `['malignant', 'benign']`, which provides a label for each target class.\n",
    "4. **`feature_names`:** A list of 30 feature names describing the measurements of the cell nuclei (e.g., 'mean radius', 'mean texture').\n",
    "5. **`DESCR`:** A detailed description of the dataset, including information about the features and the source of the dataset.\n",
    "6. **`filename`:** The path to the location of the dataset file, if applicable.\n",
    "\n",
    "These components provide all the necessary details to understand and use the dataset effectively for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b49fd2-f49b-4541-8db8-cba73acdd3fb",
   "metadata": {},
   "source": [
    "# Analysis Plan\n",
    "\n",
    "### Objective\n",
    "The objective is to estimate the logistic regression coefficients $\\widehat\\beta$, evaluate the model, and interpret the associated odds ratios.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Load the data and metadata**:\n",
    "   - Import the dataset and corresponding metadata, ensuring that data types and variable descriptions are clear.\n",
    "\n",
    "2. **Identify features and build the feature matrix (X-matrix)**:\n",
    "   - Select relevant predictor variables (features) from the dataset.\n",
    "   - Construct the feature matrix $X$ that will be used as input for the logistic regression model.\n",
    "\n",
    "3. **Identify the binary outcome (response variable)**:\n",
    "   - Define the binary response variable, $y$, which will serve as the target for the logistic regression function.\n",
    "\n",
    "4. **Variable selection**:\n",
    "   - Implement a method for feature selection (e.g., stepwise selection, LASSO, or other regularisation techniques).\n",
    "   - Differentiate between significant and insignificant features based on their contribution to the model (e.g., using p-values, AIC/BIC criteria, or cross-validation).\n",
    "\n",
    "5. **Estimate $\\widehat\\beta$ and evaluate model performance**:\n",
    "   - Fit the logistic regression model and estimate the coefficients $\\widehat\\beta$.\n",
    "   - Evaluate the performance of the model using metrics such as accuracy, precision, recall, F1-score, and the area under the ROC curve (AUC).\n",
    "\n",
    "6. **Derive odds ratios and interpret results**:\n",
    "   - Calculate the odds ratios from $\\widehat\\beta$ using the transformation $\\text{Odds Ratio} = e^{\\widehat{\\beta}}$.\n",
    "   - Interpret the odds ratios to assess the impact of each predictor on the likelihood of the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a18196-d8d9-4b69-b921-9f1789ba04f4",
   "metadata": {},
   "source": [
    "## 1. **Load the data and metadata**\n",
    "\n",
    "To load the dataset, we can use `sklearn.datasets.load_breast_cancer()`. This function loads the breast cancer dataset, which includes both the features (predictors) and the binary outcome (response). It also contains additional metadata that can help describe the dataset, such as feature names, target names, and a full dataset description accessible via the `DESCR` attribute. This information wa obtained after googling this function and reading the documentation at:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec75e2a-54b9-4048-b378-31c48b493475",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = sklearn.datasets.load_breast_cancer()\n",
    "print(cancer[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c628bdd5-29b5-46c9-99e2-2b28b044190c",
   "metadata": {},
   "source": [
    "## 2. **Identify features and build the feature matrix (X-matrix)**\n",
    "\n",
    "The `cancer.data` contains a numpy array with 569 observations (samples) and 30 features (predictors). These features represent various measurements of tumour cells. The feature matrix $X$ will consist of these 30 features for all observations.\n",
    "\n",
    "## 3. **Identify the Binary Outcome (Response Variable)**\n",
    "\n",
    "The `cancer.target` contains 569 entries, where:\n",
    "- `0` corresponds to a benign tumour,\n",
    "- `1` corresponds to a malignant tumour.\n",
    "\n",
    "This binary response will be used as the target variable $y$ for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02535795-38aa-4c61-9f06-ed7d53477cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cancer.data\n",
    "print(x.shape)\n",
    "y = cancer.target\n",
    "print(y.shape)\n",
    "print(y[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091435e-0b93-47d6-b411-5cfdcdc8a25c",
   "metadata": {},
   "source": [
    "## 4. **Variable Selection Using LASSO with Cross-Validation**\n",
    "\n",
    "### Key Steps:\n",
    "   - Use `sklearn.linear_model.LogisticRegressionCV` to perform LASSO with cross-validation on the feature matrix $X$.\n",
    "   - The cross-validation will select the optimal penalty coefficient (denoted by `C` in the pacage that corresponds to $\\lambda$) that maximises model performance, using the area under the ROC curve (AUC) as the evaluation metric.\n",
    "   - After LASSO, irrelevant features will have a coefficient of zero, and relevant features will have non-zero coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b059de6-627c-4fb2-a2bd-8ec0e989466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1 \n",
    "cv_lambdas = np.logspace(-4, 0, 100)\n",
    "\n",
    "cv_folds = 5\n",
    "lasso_cv = LogisticRegressionCV(\n",
    "    Cs=cv_lambdas,  # Number of inverse regularisation strength values to test (same as `lambda` in the lecture notes)\n",
    "    cv=cv_folds,  # 5-fold cross-validation\n",
    "    penalty='l1',  # LASSO (L1 regularisation)\n",
    "    solver='saga',  # Solver that supports L1 regularisation\n",
    "    scoring='roc_auc',  # Optimise for AUC\n",
    "    max_iter=10000,  # Allow for sufficient iterations for convergence\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "lasso_cv.fit(x, y)\n",
    "\n",
    "mean_auc_scores = np.mean(lasso_cv.scores_[1], axis=0)\n",
    "se_auc_scores = np.std(lasso_cv.scores_[1], axis=0)/np.sqrt(cv_folds-1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(\n",
    "    cv_lambdas, \n",
    "    mean_auc_scores, \n",
    "    yerr=se_auc_scores, \n",
    "    marker='o', \n",
    "    color='blue', \n",
    "    label='Mean AUC',\n",
    ")\n",
    "plt.xscale('log')  # Use log scale for C values (since C spans several magnitudes)\n",
    "plt.xlabel(r'$\\lambda$')\n",
    "plt.ylabel('Mean AUC')\n",
    "plt.title(r'AUC vs $\\lambda$ in LASSO Logistic Regression')\n",
    "plt.grid(True)\n",
    "\n",
    "max_auc_index = np.argmax(mean_auc_scores)\n",
    "max_auc = mean_auc_scores[max_auc_index]\n",
    "max_lambda = cv_lambdas[max_auc_index]\n",
    "\n",
    "plt.axvline(x=max_lambda, color='r', linestyle='--', label = fr\"$\\lambda_{{max}}={{{max_lambda:0.2}}}$\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f9c1e-afd6-43da-b578-d5b582f1f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_hat_lambda_max = lasso_cv.coef_\n",
    "beta_hat_lambda_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b199b-ac90-4da2-9e35-d47e7081fc5f",
   "metadata": {},
   "source": [
    "5. **Estimate $\\widehat\\beta$ and evaluate model performance**:\n",
    "   - Since LASSO tends to underestimate the coefficients due to the penalisation, you'll use the selected variables (non-zero coefficients) to refit a logistic regression model.\n",
    "   - This will provide a better estimate of $\\widehat\\beta$ for the significant predictors.\n",
    "   - For evaluation we will use the average AUC from a nested cross-validation evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37795ef4-50ea-4b2f-9c5a-e8aa0ad63a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.abs(lasso_cv.coef_[0]) != 0\n",
    "\n",
    "x_support = x[:, mask]\n",
    "x_support = add_constant(x_support)\n",
    "\n",
    "glm_train = sm.GLM(y , x_support, family=sm.families.Binomial())\n",
    "results = glm_train.fit()\n",
    "\n",
    "probs = results.predict(exog=x_support)\n",
    "\n",
    "beta_hat = results.params\n",
    "\n",
    "print(f\"intercept: {beta_hat[0]:.3}\")\n",
    "for coef, feature_name in zip(beta_hat[1:], cancer.feature_names[mask]):\n",
    "    print(f\"{feature_name}: {coef:.3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa77a5-3747-42f1-8a29-410cd9770b30",
   "metadata": {},
   "source": [
    "## Nested cross-validation evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c503b-fe3b-471a-881f-b44eae3b9223",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "inner_cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "lasso_pipeline = Pipeline([\n",
    "    ('logreg', LogisticRegressionCV(cv=inner_cv, penalty='l1', solver='saga', max_iter=10000, scoring='roc_auc'))  # Lasso-style regularization\n",
    "])\n",
    "\n",
    "nested_scores = cross_val_score(lasso_pipeline, x, y, cv=outer_cv, scoring='roc_auc')\n",
    "\n",
    "print(f\"Nested CV AUC Scores: {nested_scores}\")\n",
    "print(f\"Mean AUC: {np.mean(nested_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf6b646-57b2-400a-8234-73bb681d7da9",
   "metadata": {},
   "source": [
    "The mean AUC is very close to 1 therefore, these features are very good in predicting the outcome of this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6c7f31-1800-41a4-b137-152a298bc128",
   "metadata": {},
   "source": [
    "6. **Derive odds ratios and interpret results**:\n",
    "   - Calculate the odds ratios from $\\widehat\\beta$ using the transformation $\\text{Odds Ratio} = e^{\\widehat{\\beta}}$.\n",
    "   - Interpret the odds ratios to assess the impact of each predictor on the likelihood of the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb4cce-6ced-43b6-8789-f1b98825fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(beta_hat[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e77bc-46f5-43fe-bfdd-80db64e4ff42",
   "metadata": {},
   "source": [
    "### Interpreting the Odds Ratios:\n",
    "\n",
    "1. **Odds Ratio for Mean Area (1.02076911)**:\n",
    "   - The odds ratio of **$\\sim1.02$** for **mean area** shows that for each unit increase in the mean area of a tumour, the odds of having a malignant tumour increase by **2%**.\n",
    "   - This means that larger tumours, are associated with a slightly higher probability of being malignant.\n",
    "\n",
    "2. **Odds Ratio for Worst Area (0.97356275)**:\n",
    "   - The odds ratio of approximately **0.97** for **worst area** indicates that for each unit increase in the worst area of a tumour, the odds of having a malignant tumour **decrease** by **2.64%**.\n",
    "   - Contrary to the mean area, an increase in the worst area of a tumour (the area of the largest tumour detected) is associated with a **slight decrease** in the likelihood of malignancy.\n",
    "\n",
    "### General Rule for Interpreting Odds Ratios:\n",
    "- **Odds ratio > 1**: A positive association, meaning that as the predictor increases, the odds of the outcome increase.\n",
    "- **Odds ratio < 1**: A negative association, meaning that as the predictor increases, the odds of the outcome decrease.\n",
    "- **Odds ratio = 1**: No effect, meaning the predictor does not influence the odds of the outcome."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
